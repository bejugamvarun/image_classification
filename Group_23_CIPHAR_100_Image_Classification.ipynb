{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# IMPORTS AND INITIAL SETUP ALONG WITH LOADING DATASET"
      ],
      "metadata": {
        "id": "qN6ycVfB3EDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKL5meItdMRt",
        "outputId": "6ec4ab10-e778-48cf-ff7d-7eae1e8de7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:13<00:00, 12849197.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from sklearn import svm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "# Checking if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Transformations are defined\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalizing the data\n",
        "])\n",
        "\n",
        "# Augmentations for training data\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR-100 dataset\n",
        "train_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA PREPARATION"
      ],
      "metadata": {
        "id": "TI4qJk8e397X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQiQa4CktLCr",
        "outputId": "0e0c0b53-ab05-4cee-983e-15fa6b9f0e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Splitting the training data into training and validation sets\n",
        "num_train = int(len(train_data) * 0.8)\n",
        "num_val = len(train_data) - num_train\n",
        "train_dataset, val_dataset = random_split(train_data, [num_train, num_val])\n",
        "\n",
        "# DataLoader for the validation set\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=8, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=8, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EARLY STOPPING CLASS:- The `EarlyStopping` class stops training if the validation loss doesn't get better after a predetermined number of epochs, preventing overfitting. By stopping training at an appropriate time, this method reduces the amount of computational resources used while improving the model's generalization on new, unknown data."
      ],
      "metadata": {
        "id": "EP-HtsPU3qbK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acUOwm-qtLCr"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, verbose=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How many epochs to wait after last time validation accuracy improved.\n",
        "                            Default: 5\n",
        "            verbose (bool): If True, prints a message for each validation accuracy improvement.\n",
        "                            Default: False\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_acc_max = float('-inf')\n",
        "\n",
        "    def __call__(self, val_accuracy, model):\n",
        "        score = val_accuracy\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_accuracy, model)\n",
        "        elif score <= self.best_score:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_accuracy, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_accuracy, model):\n",
        "        '''Saves model when validation accuracy increase.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation accuracy increased ({self.val_acc_max:.6f} --> {val_accuracy:.6f}).  Saving model...')\n",
        "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
        "        self.val_acc_max = val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnXl-SI2tLCr"
      },
      "outputs": [],
      "source": [
        "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
        "def evaluate(model, loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "        return accuracy\n",
        "\n",
        "def train_and_validate_model(model, train_loader, val_loader, num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        val_acc = evaluate(model, val_loader)  # Evaluate on validation set\n",
        "\n",
        "        early_stopping(val_acc, model)\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN MODEL FOR CIPHAR-100"
      ],
      "metadata": {
        "id": "umjctX5k5DaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVsv1w8qtLCs",
        "outputId": "aecd57b4-02ac-4b78-b8a6-ec3795e0be73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN(\n",
            "  (conv_layer): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (21): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (23): ReLU(inplace=True)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc_layer): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=1024, out_features=512, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=512, out_features=100, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # Sequential container for convolutional layers utilizing max pooling to reduce spatial dimensions after batch normalization and ReLU activation\n",
        "        self.conv_layer = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Sequential container with dropout and ReLU activation for completely linked layers\n",
        "        self.fc_layer = nn.Sequential(\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(512 * 2 * 2, 1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, 100)  # 100 classes in CIFAR-100\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the Conv layers\n",
        "        x = self.conv_layer(x)\n",
        "        # Flatten the output for the layers which are fully connected\n",
        "        x = x.view(x.size(0), -1)\n",
        "        # FC layers are applied\n",
        "        x = self.fc_layer(x)\n",
        "        return x\n",
        "\n",
        "# Model to device\n",
        "cnn_model = CNN().to(device)\n",
        "print(cnn_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_Z1x3TjtLCs"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()  #setting up a loss function and optimizer for training a CNN model\n",
        "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL TRAINING AND VALIDATION"
      ],
      "metadata": {
        "id": "KgxI3Oje5MqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV5lwLjEtLCs",
        "outputId": "97222f33-a599-4973-8a0e-2901cae02aa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 5.01%\n",
            "Validation accuracy increased (-inf --> 5.010000).  Saving model...\n",
            "Epoch [1/50], Loss: 4.2750, Train Acc: 3.71%, Val Acc: 5.01%\n",
            "Test Accuracy: 8.46%\n",
            "Validation accuracy increased (5.010000 --> 8.460000).  Saving model...\n",
            "Epoch [2/50], Loss: 3.9784, Train Acc: 6.92%, Val Acc: 8.46%\n",
            "Test Accuracy: 11.39%\n",
            "Validation accuracy increased (8.460000 --> 11.390000).  Saving model...\n",
            "Epoch [3/50], Loss: 3.7873, Train Acc: 10.06%, Val Acc: 11.39%\n",
            "Test Accuracy: 14.68%\n",
            "Validation accuracy increased (11.390000 --> 14.680000).  Saving model...\n",
            "Epoch [4/50], Loss: 3.5888, Train Acc: 13.57%, Val Acc: 14.68%\n",
            "Test Accuracy: 18.60%\n",
            "Validation accuracy increased (14.680000 --> 18.600000).  Saving model...\n",
            "Epoch [5/50], Loss: 3.3811, Train Acc: 17.54%, Val Acc: 18.60%\n",
            "Test Accuracy: 21.39%\n",
            "Validation accuracy increased (18.600000 --> 21.390000).  Saving model...\n",
            "Epoch [6/50], Loss: 3.1732, Train Acc: 21.45%, Val Acc: 21.39%\n",
            "Test Accuracy: 26.14%\n",
            "Validation accuracy increased (21.390000 --> 26.140000).  Saving model...\n",
            "Epoch [7/50], Loss: 2.9923, Train Acc: 25.13%, Val Acc: 26.14%\n",
            "Test Accuracy: 29.26%\n",
            "Validation accuracy increased (26.140000 --> 29.260000).  Saving model...\n",
            "Epoch [8/50], Loss: 2.8127, Train Acc: 28.59%, Val Acc: 29.26%\n",
            "Test Accuracy: 31.78%\n",
            "Validation accuracy increased (29.260000 --> 31.780000).  Saving model...\n",
            "Epoch [9/50], Loss: 2.6533, Train Acc: 31.76%, Val Acc: 31.78%\n",
            "Test Accuracy: 31.69%\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [10/50], Loss: 2.5151, Train Acc: 34.11%, Val Acc: 31.69%\n",
            "Test Accuracy: 35.99%\n",
            "Validation accuracy increased (31.780000 --> 35.990000).  Saving model...\n",
            "Epoch [11/50], Loss: 2.3914, Train Acc: 37.15%, Val Acc: 35.99%\n",
            "Test Accuracy: 37.48%\n",
            "Validation accuracy increased (35.990000 --> 37.480000).  Saving model...\n",
            "Epoch [12/50], Loss: 2.2833, Train Acc: 39.20%, Val Acc: 37.48%\n",
            "Test Accuracy: 39.04%\n",
            "Validation accuracy increased (37.480000 --> 39.040000).  Saving model...\n",
            "Epoch [13/50], Loss: 2.1853, Train Acc: 41.49%, Val Acc: 39.04%\n",
            "Test Accuracy: 40.07%\n",
            "Validation accuracy increased (39.040000 --> 40.070000).  Saving model...\n",
            "Epoch [14/50], Loss: 2.0966, Train Acc: 43.70%, Val Acc: 40.07%\n",
            "Test Accuracy: 40.52%\n",
            "Validation accuracy increased (40.070000 --> 40.520000).  Saving model...\n",
            "Epoch [15/50], Loss: 2.0026, Train Acc: 45.50%, Val Acc: 40.52%\n",
            "Test Accuracy: 42.05%\n",
            "Validation accuracy increased (40.520000 --> 42.050000).  Saving model...\n",
            "Epoch [16/50], Loss: 1.9387, Train Acc: 47.02%, Val Acc: 42.05%\n",
            "Test Accuracy: 43.45%\n",
            "Validation accuracy increased (42.050000 --> 43.450000).  Saving model...\n",
            "Epoch [17/50], Loss: 1.8538, Train Acc: 48.98%, Val Acc: 43.45%\n",
            "Test Accuracy: 44.04%\n",
            "Validation accuracy increased (43.450000 --> 44.040000).  Saving model...\n",
            "Epoch [18/50], Loss: 1.7860, Train Acc: 50.60%, Val Acc: 44.04%\n",
            "Test Accuracy: 44.15%\n",
            "Validation accuracy increased (44.040000 --> 44.150000).  Saving model...\n",
            "Epoch [19/50], Loss: 1.7176, Train Acc: 52.22%, Val Acc: 44.15%\n",
            "Test Accuracy: 44.78%\n",
            "Validation accuracy increased (44.150000 --> 44.780000).  Saving model...\n",
            "Epoch [20/50], Loss: 1.6634, Train Acc: 53.62%, Val Acc: 44.78%\n",
            "Test Accuracy: 45.74%\n",
            "Validation accuracy increased (44.780000 --> 45.740000).  Saving model...\n",
            "Epoch [21/50], Loss: 1.6132, Train Acc: 55.03%, Val Acc: 45.74%\n",
            "Test Accuracy: 45.32%\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [22/50], Loss: 1.5482, Train Acc: 56.42%, Val Acc: 45.32%\n",
            "Test Accuracy: 46.98%\n",
            "Validation accuracy increased (45.740000 --> 46.980000).  Saving model...\n",
            "Epoch [23/50], Loss: 1.4901, Train Acc: 57.82%, Val Acc: 46.98%\n",
            "Test Accuracy: 46.35%\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [24/50], Loss: 1.4603, Train Acc: 58.54%, Val Acc: 46.35%\n",
            "Test Accuracy: 47.09%\n",
            "Validation accuracy increased (46.980000 --> 47.090000).  Saving model...\n",
            "Epoch [25/50], Loss: 1.4138, Train Acc: 59.44%, Val Acc: 47.09%\n",
            "Test Accuracy: 47.01%\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [26/50], Loss: 1.3574, Train Acc: 60.99%, Val Acc: 47.01%\n",
            "Test Accuracy: 47.98%\n",
            "Validation accuracy increased (47.090000 --> 47.980000).  Saving model...\n",
            "Epoch [27/50], Loss: 1.3263, Train Acc: 61.79%, Val Acc: 47.98%\n",
            "Test Accuracy: 48.28%\n",
            "Validation accuracy increased (47.980000 --> 48.280000).  Saving model...\n",
            "Epoch [28/50], Loss: 1.2866, Train Acc: 63.16%, Val Acc: 48.28%\n",
            "Test Accuracy: 48.46%\n",
            "Validation accuracy increased (48.280000 --> 48.460000).  Saving model...\n",
            "Epoch [29/50], Loss: 1.2540, Train Acc: 63.63%, Val Acc: 48.46%\n",
            "Test Accuracy: 47.14%\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [30/50], Loss: 1.2198, Train Acc: 64.79%, Val Acc: 47.14%\n",
            "Test Accuracy: 48.43%\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch [31/50], Loss: 1.1875, Train Acc: 65.69%, Val Acc: 48.43%\n",
            "Test Accuracy: 48.62%\n",
            "Validation accuracy increased (48.460000 --> 48.620000).  Saving model...\n",
            "Epoch [32/50], Loss: 1.1453, Train Acc: 66.65%, Val Acc: 48.62%\n",
            "Test Accuracy: 49.31%\n",
            "Validation accuracy increased (48.620000 --> 49.310000).  Saving model...\n",
            "Epoch [33/50], Loss: 1.1313, Train Acc: 66.92%, Val Acc: 49.31%\n",
            "Test Accuracy: 48.71%\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [34/50], Loss: 1.1003, Train Acc: 67.74%, Val Acc: 48.71%\n",
            "Test Accuracy: 49.72%\n",
            "Validation accuracy increased (49.310000 --> 49.720000).  Saving model...\n",
            "Epoch [35/50], Loss: 1.0608, Train Acc: 68.67%, Val Acc: 49.72%\n",
            "Test Accuracy: 50.16%\n",
            "Validation accuracy increased (49.720000 --> 50.160000).  Saving model...\n",
            "Epoch [36/50], Loss: 1.0453, Train Acc: 69.35%, Val Acc: 50.16%\n",
            "Test Accuracy: 50.06%\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [37/50], Loss: 1.0189, Train Acc: 70.00%, Val Acc: 50.06%\n",
            "Test Accuracy: 48.76%\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch [38/50], Loss: 0.9744, Train Acc: 71.17%, Val Acc: 48.76%\n",
            "Test Accuracy: 49.30%\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch [39/50], Loss: 0.9662, Train Acc: 71.38%, Val Acc: 49.30%\n",
            "Test Accuracy: 49.70%\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch [40/50], Loss: 0.9412, Train Acc: 72.32%, Val Acc: 49.70%\n",
            "Test Accuracy: 50.04%\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "train_and_validate_model(cnn_model, train_loader, val_loader, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRcNKgihtLCs"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False) #First layer of convolution with a certain stride\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels) #Batch normalization after the first convolution\n",
        "        self.relu = nn.ReLU(inplace = True) #ReLU activation function\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False) #Second layer of convolution with stride 1, without bias\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels) #Batch normalization\n",
        "        self.downsample = downsample #downsample to adjust channels and dimensions\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x) #convulation 1 operation\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out) #convulation 2 operation\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RESNET MODEL FOR CIPHAR-100"
      ],
      "metadata": {
        "id": "fHWcqJFj8V51"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tExwxnpWtLCs"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet, self).__init__()\n",
        "        # Set the initial convolution, batch normalization, and input channel initialization\n",
        "        self.in_channels = 64\n",
        "        self.conv = nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        # Layers of residual blocks are defined\n",
        "        self.layer1 = self._make_layer(64, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1)) # Pooling that adapts to output size\n",
        "        self.fc = nn.Linear(512, 100) # Final linear layer for classification\n",
        "\n",
        "    def _make_layer(self, out_channels, blocks, stride):\n",
        "        downsample = None  # If necessary, configure the downsample for the remaining connection.\n",
        "        if stride != 1 or self.in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        layers = []\n",
        "\n",
        "        # Creating blocks of residual layers\n",
        "        layers.append(ResidualBlock(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):  # Activation, normalization, and initial convolution\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juhA-Lq_tLCs",
        "outputId": "8e863560-84e7-44ae-8828-6ac67c077159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): ResidualBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): ResidualBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "resnet_model = ResNet().to(device)\n",
        "optimizer = optim.Adam(resnet_model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "print(resnet_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duYyBTRztLCt",
        "outputId": "35f143fc-9323-4832-ac34-73b0be5dd34f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 20.08%\n",
            "Validation accuracy increased (-inf --> 20.080000).  Saving model...\n",
            "Epoch [1/50], Loss: 3.4678, Train Acc: 16.29%, Val Acc: 20.08%\n",
            "Test Accuracy: 23.60%\n",
            "Validation accuracy increased (20.080000 --> 23.600000).  Saving model...\n",
            "Epoch [2/50], Loss: 3.2092, Train Acc: 21.04%, Val Acc: 23.60%\n",
            "Test Accuracy: 28.74%\n",
            "Validation accuracy increased (23.600000 --> 28.740000).  Saving model...\n",
            "Epoch [3/50], Loss: 2.8816, Train Acc: 27.28%, Val Acc: 28.74%\n",
            "Test Accuracy: 33.22%\n",
            "Validation accuracy increased (28.740000 --> 33.220000).  Saving model...\n",
            "Epoch [4/50], Loss: 2.6505, Train Acc: 32.20%, Val Acc: 33.22%\n",
            "Test Accuracy: 35.47%\n",
            "Validation accuracy increased (33.220000 --> 35.470000).  Saving model...\n",
            "Epoch [5/50], Loss: 2.4525, Train Acc: 36.23%, Val Acc: 35.47%\n",
            "Test Accuracy: 37.06%\n",
            "Validation accuracy increased (35.470000 --> 37.060000).  Saving model...\n",
            "Epoch [6/50], Loss: 2.2686, Train Acc: 40.58%, Val Acc: 37.06%\n",
            "Test Accuracy: 39.60%\n",
            "Validation accuracy increased (37.060000 --> 39.600000).  Saving model...\n",
            "Epoch [7/50], Loss: 2.1272, Train Acc: 43.44%, Val Acc: 39.60%\n",
            "Test Accuracy: 42.43%\n",
            "Validation accuracy increased (39.600000 --> 42.430000).  Saving model...\n",
            "Epoch [8/50], Loss: 1.9731, Train Acc: 46.69%, Val Acc: 42.43%\n",
            "Test Accuracy: 43.75%\n",
            "Validation accuracy increased (42.430000 --> 43.750000).  Saving model...\n",
            "Epoch [9/50], Loss: 1.8404, Train Acc: 49.79%, Val Acc: 43.75%\n",
            "Test Accuracy: 45.91%\n",
            "Validation accuracy increased (43.750000 --> 45.910000).  Saving model...\n",
            "Epoch [10/50], Loss: 1.7385, Train Acc: 52.28%, Val Acc: 45.91%\n",
            "Test Accuracy: 47.04%\n",
            "Validation accuracy increased (45.910000 --> 47.040000).  Saving model...\n",
            "Epoch [11/50], Loss: 1.6198, Train Acc: 54.81%, Val Acc: 47.04%\n",
            "Test Accuracy: 48.26%\n",
            "Validation accuracy increased (47.040000 --> 48.260000).  Saving model...\n",
            "Epoch [12/50], Loss: 1.5100, Train Acc: 57.42%, Val Acc: 48.26%\n",
            "Test Accuracy: 48.46%\n",
            "Validation accuracy increased (48.260000 --> 48.460000).  Saving model...\n",
            "Epoch [13/50], Loss: 1.4090, Train Acc: 59.50%, Val Acc: 48.46%\n",
            "Test Accuracy: 49.17%\n",
            "Validation accuracy increased (48.460000 --> 49.170000).  Saving model...\n",
            "Epoch [14/50], Loss: 1.3135, Train Acc: 62.27%, Val Acc: 49.17%\n",
            "Test Accuracy: 49.59%\n",
            "Validation accuracy increased (49.170000 --> 49.590000).  Saving model...\n",
            "Epoch [15/50], Loss: 1.2270, Train Acc: 64.27%, Val Acc: 49.59%\n",
            "Test Accuracy: 50.23%\n",
            "Validation accuracy increased (49.590000 --> 50.230000).  Saving model...\n",
            "Epoch [16/50], Loss: 1.1529, Train Acc: 66.28%, Val Acc: 50.23%\n",
            "Test Accuracy: 50.32%\n",
            "Validation accuracy increased (50.230000 --> 50.320000).  Saving model...\n",
            "Epoch [17/50], Loss: 1.0637, Train Acc: 68.17%, Val Acc: 50.32%\n",
            "Test Accuracy: 50.20%\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [18/50], Loss: 0.9951, Train Acc: 70.28%, Val Acc: 50.20%\n",
            "Test Accuracy: 51.54%\n",
            "Validation accuracy increased (50.320000 --> 51.540000).  Saving model...\n",
            "Epoch [19/50], Loss: 0.9295, Train Acc: 72.09%, Val Acc: 51.54%\n",
            "Test Accuracy: 51.53%\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [20/50], Loss: 0.8569, Train Acc: 73.90%, Val Acc: 51.53%\n",
            "Test Accuracy: 50.90%\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch [21/50], Loss: 0.8024, Train Acc: 75.42%, Val Acc: 50.90%\n",
            "Test Accuracy: 50.20%\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch [22/50], Loss: 0.7539, Train Acc: 76.80%, Val Acc: 50.20%\n",
            "Test Accuracy: 50.40%\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch [23/50], Loss: 0.6967, Train Acc: 78.41%, Val Acc: 50.40%\n",
            "Test Accuracy: 51.31%\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "train_and_validate_model(resnet_model, train_loader, val_loader, 50)  # model training and validation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG MODEL FOR CIPHAR-100"
      ],
      "metadata": {
        "id": "I_eV8aEL_cBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfkDYGiftLCt"
      },
      "outputs": [],
      "source": [
        "class VGG16CIFAR100(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VGG16CIFAR100, self).__init__()\n",
        "        # Setting up VGG-like layers with max pooling ('M') after convolutions\n",
        "        self.features = self._make_layers([\n",
        "            64, 64, 'M',      # 'M' stands for MaxPool\n",
        "            128, 128, 'M',\n",
        "            256, 256, 256, 'M',\n",
        "            512, 512, 512, 'M',\n",
        "            512, 512, 512, 'M'\n",
        "        ])\n",
        "        # Fully connected layers are used for classification, while dropout is used for regularization in the classifier.\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 100)  # CIFAR-100 has 100 classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x) # Through feature extractor input is passed and output is flattend in order to feed into the classifier\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3  # Input channels (RGB)\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        return nn.Sequential(*layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yewfkPYtLCt",
        "outputId": "cb33af06-18e9-4f4f-a2c6-0475869697e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VGG16CIFAR100(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "    (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (12): ReLU(inplace=True)\n",
            "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (16): ReLU(inplace=True)\n",
            "    (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (19): ReLU(inplace=True)\n",
            "    (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (26): ReLU(inplace=True)\n",
            "    (27): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (32): ReLU(inplace=True)\n",
            "    (33): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (35): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (36): ReLU(inplace=True)\n",
            "    (37): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (38): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (39): ReLU(inplace=True)\n",
            "    (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (42): ReLU(inplace=True)\n",
            "    (43): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Dropout(p=0.5, inplace=False)\n",
            "    (1): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): Dropout(p=0.5, inplace=False)\n",
            "    (4): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): Linear(in_features=512, out_features=100, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "vgg_model = VGG16CIFAR100().to(device)  # Set up the VGG16 model that was modified to work with the CIFAR-100 dataset\n",
        "print(vgg_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-XJfGmHtLCt",
        "outputId": "78e30598-f11f-4c8d-d05a-8acb1f4fb99d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.14%\n",
            "Validation accuracy increased (-inf --> 1.140000).  Saving model...\n",
            "Epoch [1/50], Loss: 4.6330, Train Acc: 1.00%, Val Acc: 1.14%\n",
            "Test Accuracy: 1.16%\n",
            "Validation accuracy increased (1.140000 --> 1.160000).  Saving model...\n",
            "Epoch [2/50], Loss: 4.6105, Train Acc: 1.00%, Val Acc: 1.16%\n",
            "Test Accuracy: 0.99%\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [3/50], Loss: 4.6105, Train Acc: 1.08%, Val Acc: 0.99%\n",
            "Test Accuracy: 0.96%\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch [4/50], Loss: 4.6106, Train Acc: 1.06%, Val Acc: 0.96%\n",
            "Test Accuracy: 1.13%\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch [5/50], Loss: 4.6108, Train Acc: 1.02%, Val Acc: 1.13%\n",
            "Test Accuracy: 1.09%\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch [6/50], Loss: 4.6107, Train Acc: 0.99%, Val Acc: 1.09%\n",
            "Test Accuracy: 1.24%\n",
            "Validation accuracy increased (1.160000 --> 1.240000).  Saving model...\n",
            "Epoch [7/50], Loss: 4.6110, Train Acc: 1.02%, Val Acc: 1.24%\n",
            "Test Accuracy: 1.18%\n",
            "EarlyStopping counter: 1 out of 5\n",
            "Epoch [8/50], Loss: 4.6104, Train Acc: 1.07%, Val Acc: 1.18%\n",
            "Test Accuracy: 0.97%\n",
            "EarlyStopping counter: 2 out of 5\n",
            "Epoch [9/50], Loss: 4.6109, Train Acc: 0.96%, Val Acc: 0.97%\n",
            "Test Accuracy: 0.96%\n",
            "EarlyStopping counter: 3 out of 5\n",
            "Epoch [10/50], Loss: 4.6106, Train Acc: 0.99%, Val Acc: 0.96%\n",
            "Test Accuracy: 1.12%\n",
            "EarlyStopping counter: 4 out of 5\n",
            "Epoch [11/50], Loss: 4.6105, Train Acc: 1.10%, Val Acc: 1.12%\n",
            "Test Accuracy: 1.12%\n",
            "EarlyStopping counter: 5 out of 5\n",
            "Early stopping\n"
          ]
        }
      ],
      "source": [
        "train_and_validate_model(vgg_model, train_loader, val_loader, 50)  # model training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIth_sl0tLCt"
      },
      "outputs": [],
      "source": [
        "# Convert to numpy arrays for compatibility with scikit-learn\n",
        "def convert_to_numpy(dataset):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for image, label in dataset:\n",
        "        image = image.permute(1, 2, 0).numpy()  # Reorder dimensions to HWC\n",
        "        image = image.flatten()  # Flatten the image\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "X_train, y_train = convert_to_numpy(train_data)\n",
        "X_test, y_test = convert_to_numpy(test_data)\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM MODEL TRAINING FOR CIPHAR-100"
      ],
      "metadata": {
        "id": "S6jLucL-_kHe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTEHOujJtLCt",
        "outputId": "4fe7bccb-918c-4ddb-f9c8-6c4fdf49152a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM with linear kernel...\n",
            "Validation accuracy for linear kernel: 0.0856\n",
            "Training SVM with rbf kernel...\n"
          ]
        }
      ],
      "source": [
        "# Function to train and evaluate SVM\n",
        "def train_svm(kernel_type):\n",
        "    print(f\"Training SVM with {kernel_type} kernel...\")\n",
        "    svm_model = svm.SVC(kernel=kernel_type)\n",
        "    svm_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict and evaluate on the validation set\n",
        "    y_val_pred = svm_model.predict(X_val_scaled)\n",
        "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    print(f\"Validation accuracy for {kernel_type} kernel: {val_accuracy:.4f}\")\n",
        "\n",
        "# Train SVM with linear kernel\n",
        "train_svm('linear')\n",
        "\n",
        "# Train SVM with radial kernel\n",
        "train_svm('rbf')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fMnRZtgDtcsM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}